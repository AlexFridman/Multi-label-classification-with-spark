{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = sc.textFile('hdfs://master:54310/single-label'). \\\n",
    "        map(lambda line: json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_w = data.map(lambda x: len(x['Features'])).filter(lambda x: x==0)\n",
    "zero_w.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excluding_words = data.flatMap(lambda d: set(d['Features'])). \\\n",
    "    map(lambda d: (d,1)). \\\n",
    "    reduceByKey(int.__add__). \\\n",
    "    filter(lambda f: f[1] == 1). \\\n",
    "    map(lambda f: f[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excluding_words_set = set(excluding_words)\n",
    "excluding_words_set_br = sc.broadcast(excluding_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exclude_words(doc):\n",
    "    doc['Features'] = [w for w in doc['Features'] if w not in excluding_words_set_br.value]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_clean_words = data.map(exclude_words).filter(lambda doc: doc['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_idx = data_clean_words.flatMap(lambda doc: set(doc['Features'])). \\\n",
    "            distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_idx = data_clean_words.map(lambda doc: doc['Label']). \\\n",
    "            distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile('/home/hadoop/spark/lib/sparse.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(features_idx)\n",
    "feature_idx_br = sc.broadcast(features_idx)\n",
    "label_idx_br = sc.broadcast(labels_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sparse import sparse_vector\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def vectorize_words(words: list):\n",
    "    word_counts = Counter([feature_idx_br.value[w] for w in words])\n",
    "    return sparse_vector(list(word_counts.items()), length=num_features, dtype=np.int32)\n",
    "\n",
    "def vectorize_data(x: dict):\n",
    "    features = vectorize_words(x['Features'])\n",
    "    lables = label_idx_br.value[x['Label']]\n",
    "    return {'lable':lables, 'features':features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect_data = data_clean_words.map(vectorize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vd = vect_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_count = len(vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dok_matrix((doc_count, num_features), dtype=np.int32)\n",
    "y = np.zeros(doc_count, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "20000\n",
      "22000\n",
      "24000\n",
      "26000\n",
      "28000\n",
      "30000\n",
      "32000\n",
      "34000\n",
      "36000\n",
      "38000\n",
      "40000\n",
      "42000\n",
      "44000\n",
      "46000\n",
      "48000\n",
      "50000\n",
      "52000\n",
      "54000\n",
      "56000\n",
      "58000\n",
      "60000\n",
      "62000\n",
      "64000\n",
      "66000\n",
      "68000\n",
      "70000\n",
      "72000\n",
      "74000\n"
     ]
    }
   ],
   "source": [
    "for row_num, doc in enumerate(vd):\n",
    "    if row_num % 2000 == 0:\n",
    "        print(row_num)\n",
    "    label = doc['lable']\n",
    "    features = doc['features']\n",
    "    y[row_num] = label\n",
    "#     X[row_num,:] = features.inner\n",
    "    for col_num in features.nonzero():\n",
    "        X[row_num, col_num] = features[col_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/hadoop/exp/label_idx.json', 'w+') as fp:\n",
    "    json.dump(labels_idx, fp, ensure_ascii=False)\n",
    "with open('/home/hadoop/exp/feature_idx.json', 'w+') as fp:\n",
    "    json.dump(features_idx, fp, ensure_ascii=False)\n",
    "from scipy import io\n",
    "io.mmwrite('/home/hadoop/exp/X.mtx', X)\n",
    "np.save('/home/hadoop/exp/y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
