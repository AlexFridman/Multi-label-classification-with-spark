{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "raw_data = sc.textFile('hdfs://master:54310/single-label'). \\\n",
    "    map(lambda line: json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excluding_wrods = raw_data.flatMap(lambda doc: [(f,1) for f in doc['Features']]). \\\n",
    "    reduceByKey(lambda a,b: a+b). \\\n",
    "    filter(lambda wc: wc[1] < 6). \\\n",
    "    collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exclude_words(obj: dict):\n",
    "    obj['Features'] = [w for w in obj['Features'] if w not in excluding_wrods]\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = raw_data.map(lambda x: exclude_words(x)).filter(lambda x: x['Features'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# data.filter(lambda x: x['Features']).map(lambda x: json.dumps(x)).repartition(4).saveAsTextFile('hdfs://master:54310/filtered')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "raw_data.map(lambda x: len(x['Features'])).mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "data.map(lambda x: len(x['Features'])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_idx = data.flatMap(lambda doc: doc['Features']). \\\n",
    "        distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_num = len(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_idx = data.map(lambda doc: doc['Label']). \\\n",
    "        distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LabeledPoint\n",
    "from collections import Counter          \n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_obj_to_lpoint(obj: dict):\n",
    "    label = label_idx[obj['Label']]\n",
    "    word_count = Counter(obj['Features'])\n",
    "    feature_vec = Vectors.sparse(features_num, [(word_idx[wc[0]], wc[1]) \n",
    "                                                for wc in dict(word_count).items() if wc[0] in word_idx])\n",
    "    return LabeledPoint(label, feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = data.map(lambda obj: map_obj_to_lpoint(obj))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "all_data.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import sys\n",
    "all_data.map(lambda x: sys.getsizeof(x)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Размерность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119377"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.take(1)[0].features.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "training, test = all_data.randomSplit([0.6, 0.4], seed=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "model = NaiveBayes.train(training)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test.map(lambda x: model.predict(x.features) == x.label).count() / test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[19] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def hold_out(data, k):\n",
    "    data_count = data.count()\n",
    "    print('data count', data_count)\n",
    "    partition_num = data.getNumPartitions()\n",
    "    # shuffle\n",
    "    idxs = list(range(data_count))\n",
    "    random.shuffle(idxs)\n",
    "    idxs_rdd = sc.parallelize(idxs).repartition(partition_num)\n",
    "    shuffled = idxs_rdd.keyBy(lambda x: x).join(data.zipWithIndex().map(lambda x: (x[1], x[0]))). \\\n",
    "        sortByKey().map(lambda x: x[1][1])\n",
    "    \n",
    "    sum_of_pred_accurace = 0\n",
    "    \n",
    "    h = data_count // k\n",
    "    idxs = range(0, data_count, h)\n",
    "    print(list(idxs))\n",
    "    indexed_data = shuffled.zipWithIndex()\n",
    "    for i, (l,r) in enumerate(zip(idxs, idxs[1:])):\n",
    "        print('#' + str(i))\n",
    "        print('l:',l,'r:',r)\n",
    "        test = indexed_data.filter(lambda x: l <= x[1] < r).map(lambda x: x[0])\n",
    "        training = indexed_data.filter(lambda x: x[1] < l or x[1] >= r).map(lambda x: x[0])\n",
    "        \n",
    "        model = NaiveBayes.train(training)\n",
    "        predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\n",
    "        accurace = 1.0 * predictionAndLabel.filter(lambda x: x[0] == x[1]).count() / (r - l) * 100\n",
    "        print(accurace)\n",
    "        sum_of_pred_accurace += accurace\n",
    "    result = sum_of_pred_accurace.value / k\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hold_out(all_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data count 82106\n"
     ]
    }
   ],
   "source": [
    "data_count = all_data.count()\n",
    "print('data count', data_count)\n",
    "partition_num = all_data.getNumPartitions()\n",
    "# shuffle\n",
    "idxs = list(range(data_count))\n",
    "random.shuffle(idxs)\n",
    "idxs_rdd = sc.parallelize(idxs).repartition(partition_num)\n",
    "shuffled = idxs_rdd.keyBy(lambda x: x).join(all_data.zipWithIndex().map(lambda x: (x[1], x[0]))). \\\n",
    "    sortByKey().map(lambda x: x[1][1])\n",
    "training, test = shuffled.randomSplit([0.6, 0.4], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NaiveBayes.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100.0 * test.map(lambda x: model.predict(x.features) == x.label).count() / test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(sc, 'hdfs://master:54310/bayes-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model, open('/home/hadoop/models/bayes/bayes.p', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
