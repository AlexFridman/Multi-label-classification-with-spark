{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import RDD\n",
    "import json\n",
    "from pyspark.mllib.feature import IDF\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StoringTF(object):\n",
    "    def fit(self, vacabulary):\n",
    "        self.word_idx = vacabulary.distinct().zipWithIndex().collectAsMap()\n",
    "        self.numFeatures = len(self.word_idx)\n",
    "        \n",
    "    def indexOf(self, term):\n",
    "        \"\"\" Returns the index of the input term. \"\"\"\n",
    "        return self.word_idx[term]\n",
    "\n",
    "    def transform(self, document):\n",
    "        \"\"\"\n",
    "        Transforms the input document (list of terms) to term frequency\n",
    "        vectors, or transform the RDD of document to RDD of term\n",
    "        frequency vectors.\n",
    "        \"\"\"\n",
    "        if isinstance(document, RDD):\n",
    "            return document.map(self.transform)\n",
    "\n",
    "        freq = {}\n",
    "        for term in document:\n",
    "            i = self.indexOf(term)\n",
    "            freq[i] = freq.get(i, 0) + 1.0\n",
    "        return Vectors.sparse(self.numFeatures, freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile('hdfs://master:54310/exp_2/ml_data'). \\\n",
    "    map(lambda line: json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idx = raw_data.flatMap(lambda doc: set(doc['Features'])).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idx_inv = dict([(v,k) for k,v in word_idx.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/hadoop/data/w_i_ex_2.json', 'w+') as fp:\n",
    "    json.dump(word_idx, fp)\n",
    "with open('/home/hadoop/data/w_i_inv_ex_2.json', 'w+') as fp:\n",
    "    json.dump(word_idx_inv, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/hadoop/data/w_i_ex_2.json', 'r') as fp:\n",
    "    word_idx = json.load(fp)\n",
    "with open('/home/hadoop/data/w_i_inv_ex_2.json', 'r') as fp:\n",
    "    word_idx_inv = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idx_br = sc.broadcast(word_idx)\n",
    "word_idx_inv_br = sc.broadcast(word_idx_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vect_f(doc: dict):\n",
    "    doc['Features'] = [word_idx_br.value[w] for w in doc['Features']]\n",
    "    return doc\n",
    "\n",
    "vect_raw_data = raw_data.map(vect_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_excl_unigramms_br = sc.broadcast(set(vect_raw_data.flatMap(lambda x: set(x['Features'])). \\\n",
    "    map(lambda f: (f, 1)). \\\n",
    "    reduceByKey(int.__add__). \\\n",
    "    filter(lambda x: x[1] > 14).map(lambda x: x[0]).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_unigramms(doc: dict):\n",
    "    unigramms = [word_idx_inv_br.value[ug] for ug in doc['Features'] if ug in not_excl_unigramms_br.value]\n",
    "    return {'Id': doc['Id'], 'Unigramms': unigramms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect_raw_data.map(filter_unigramms).map(json.dumps).repartition(6).saveAsTextFile('hdfs://master:54310/exp_2/unigramms')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def add_bi_and_tri_gramms(doc: dict):\n",
    "    words = doc['Features']\n",
    "    bigramms = [(w_1, w_2) for w_1, w_2 in zip(words[:-1], words[1:])]\n",
    "    trigramms = [(w_1, w_2, w_3) for w_1, w_2, w_3 in zip(words[:-2], words[1:-1], words[2:])]\n",
    "    doc['Features'].extend(bigramms)\n",
    "    doc['Features'].extend(trigramms)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigramms(doc: dict):\n",
    "    words = doc['Features']\n",
    "    bigramms = [(w_1, w_2) for w_1, w_2 in zip(words[:-1], words[1:])]\n",
    "    return {'Id': doc['Id'], 'Bigramms': bigramms}\n",
    "\n",
    "def get_trigramms(doc: dict):\n",
    "    words = doc['Features']\n",
    "    trigramms = [(w_1, w_2, w_3) for w_1, w_2, w_3 in zip(words[:-2], words[1:-1], words[2:])]\n",
    "    return {'Id': doc['Id'], 'Trigramms': trigramms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigramms = vect_raw_data.map(get_bigramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigramms = vect_raw_data.map(get_trigramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112807"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigramms.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_excl_bigramms = set(bigramms.flatMap(lambda doc: set(doc['Bigramms'])). \\\n",
    "    map(lambda f: (f, 1)). \\\n",
    "    reduceByKey(int.__add__). \\\n",
    "    filter(lambda fc: fc[1] > 69). \\\n",
    "    map(lambda fc: fc[0]).collect())\n",
    "\n",
    "not_excl_bigramms_br = sc.broadcast(not_excl_bigramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49054"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_excl_bigramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_excl_trigramms_br = sc.broadcast(set(trigramms.flatMap(lambda doc: set(doc['Trigramms'])). \\\n",
    "    map(lambda f: (f, 1)). \\\n",
    "    reduceByKey(int.__add__). \\\n",
    "    filter(lambda fc: fc[1] > 14). \\\n",
    "    map(lambda fc: fc[0]).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38606"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_excl_trigramms_br.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_bigramms(doc: dict):\n",
    "    doc['Bigramms'] = ['{} {}'.format(word_idx_inv_br.value[bg[0]], word_idx_inv_br.value[bg[1]])\n",
    "                       for bg in doc['Bigramms'] if bg in not_excl_bigramms_br.value]\n",
    "    \n",
    "    return doc\n",
    "\n",
    "bigramms.map(filter_bigramms). \\\n",
    "    map(json.dumps). \\\n",
    "    repartition(6). \\\n",
    "    saveAsTextFile('hdfs://master:54310/exp_2/bigramms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_trigramms(doc: dict):\n",
    "    doc['Trigramms'] = ['{} {} {}'.format(word_idx_inv_br.value[tg[0]],\n",
    "                                          word_idx_inv_br.value[tg[1]], word_idx_inv_br.value[tg[2]])\n",
    "                       for tg in doc['Trigramms'] if tg in not_excl_trigramms_br.value]\n",
    "    \n",
    "    return doc\n",
    "\n",
    "trigramms.map(filter_trigramms). \\\n",
    "    map(json.dumps). \\\n",
    "    repartition(6). \\\n",
    "    saveAsTextFile('hdfs://master:54310/exp_2/trigramms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigramms = sc.textFile('hdfs://master:54310/exp_2/unigramms'). \\\n",
    "    map(json.loads).map(lambda x: (x['Id'], x['Unigramms']))\n",
    "bigramms = sc.textFile('hdfs://master:54310/exp_2/bigramms'). \\\n",
    "    map(json.loads).map(lambda x: (x['Id'], x['Bigramms']))\n",
    "trigramms = sc.textFile('hdfs://master:54310/exp_2/trigramms'). \\\n",
    "    map(json.loads).map(lambda x: (x['Id'], x['Trigramms']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = sc.union([unigramms, bigramms, trigramms]). \\\n",
    "    reduceByKey(list.__add__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = raw_data.map(lambda x: (x['Id'],x['Labels']))\n",
    "data = labels.join(features).map(lambda x: {'Id':x[0],'Labels':x[1][0],'Features':x[1][1]})\n",
    "data.map(json.dumps).repartition(6). \\\n",
    "    saveAsTextFile('hdfs://master:54310/exp_2/clean_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile('hdfs://master:54310/new_lables'). \\\n",
    "    map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_1000_labels = dict(data.flatMap(lambda x: x['Labels']). \\\n",
    "    map(lambda l: (l, 1)). \\\n",
    "    reduceByKey(int.__add__). \\\n",
    "    sortBy(lambda lc: lc[1], ascending=False). \\\n",
    "    take(800))\n",
    "\n",
    "top_1000_labels_br = sc.broadcast(top_1000_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_labels(doc: dict):\n",
    "    doc['Labels'] = [l for l in doc['Labels'] if l in top_1000_labels_br.value]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data = data.map(filter_labels).filter(lambda x: len(x['Labels']) > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = clean_data.map(lambda x: x['Features'])\n",
    "vocabulary = documents.flatMap(lambda d: d).distinct()\n",
    "storingTF = StoringTF()\n",
    "storingTF.fit(vocabulary)\n",
    "tf = storingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(minDocFreq=3).fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64781"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tfidf.take(1)[0]\n",
    "t.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = clean_data.map(lambda x: x['Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_data = tfidf.zip(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def to_tf_idf(data: RDD):\n",
    "    documents = data.map(lambda x: x['Features'])\n",
    "    vocabulary = documents.flatMap(lambda d: d).distinct()\n",
    "    storingTF = StoringTF()\n",
    "    storingTF.fit(vocabulary)\n",
    "    tf = storingTF.transform(documents)\n",
    "    idf = IDF().fit(tf)\n",
    "    tfidf = idf.transform(tf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import json\n",
    "from pyspark import RDD\n",
    "import numpy as np\n",
    "sc.addPyFile('/home/hadoop/spark/lib/sparse.py')\n",
    "sc.addPyFile('/home/hadoop/spark/lib/model.py')\n",
    "from sparse import sparse_vector\n",
    "from model import MLNaiveBayesModel\n",
    "def shuffle_and_split(data: RDD, fold_n: int, seed = 0):\n",
    "    fold_weights = [1 / fold_n] * fold_n\n",
    "    return data.randomSplit(fold_weights)\n",
    "\n",
    "def hold_out(sc, data: RDD, k: int, model_builder, metrics: list):\n",
    "    folds = shuffle_and_split(data, k)\n",
    "    for i in range(k):\n",
    "        test = folds[i]\n",
    "        training = sc.union(folds[:i] + folds[i + 1:])\n",
    "        model = model_builder(training)\n",
    "        model_broadcast = sc.broadcast(model)\n",
    "        lables_and_predictions = test.map(lambda x: (x['lables'],\n",
    "                                      model_broadcast.value.predict_all(x['features'])))\n",
    "        for metric in metrics:\n",
    "            metric.evaluate(lables_and_predictions)\n",
    "    return metrics\n",
    "\n",
    "class Metric:\n",
    "    def __init__(self, name: str, verbose=False):\n",
    "        self._name = name\n",
    "        self._results = []\n",
    "        self._verbose = verbose\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def results(self):\n",
    "        return self._results\n",
    "    \n",
    "    @property\n",
    "    def avg(self):\n",
    "        return np.average(self._results)\n",
    "    \n",
    "    def evaluate(self, lables, predictions):\n",
    "        pass\n",
    "\n",
    "class AccuracyMetric(Metric):\n",
    "    def __init__(self, pred_n: int, intersect_n: int):\n",
    "        self._pred_n = pred_n\n",
    "        self._intersect_n = intersect_n\n",
    "        super(AccuracyMetric, self).__init__(name='Accuracy', verbose=False)\n",
    "        \n",
    "    def evaluate(self, lables_and_predictions: RDD):\n",
    "        TP = lables_and_predictions.map(lambda x:\n",
    "                                    (set(x[0]), set([p for p,w in x[1][:self._pred_n]]))). \\\n",
    "                                    filter(lambda x:\n",
    "                                           len(x[0].intersection(x[1])) >= self._intersect_n)\n",
    "        accuracy = 100.0 * TP.count() / lables_and_predictions.count()\n",
    "        if self._verbose:\n",
    "            print('accuracy: ', accuracy)\n",
    "        self._results.append(accuracy)\n",
    "        return accuracy\n",
    "\n",
    "from pyspark.mllib.classification import NaiveBayesModel\n",
    "from pyspark.mllib.linalg import _convert_to_vector\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import RDD\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# RDD (labels) (features)\n",
    "def train_model(data, l = 1.0):\n",
    "    aggreagated = data.flatMap(lambda x: [(l, x['features']) for l in x['lables']]). \\\n",
    "        combineByKey(lambda v: (1, v),\n",
    "                 lambda c, v: (c[0] + 1, c[1] + v),\n",
    "                 lambda c1, c2: (c1[0] + c2[0], c1[1] + c2[1])). \\\n",
    "        sortBy(lambda x: x[0]). \\\n",
    "        collect()\n",
    "    num_labels = len(aggreagated)\n",
    "    num_documents = data.count()\n",
    "    num_features = aggreagated[0][1][1].size\n",
    "    labels = np.zeros(num_labels)\n",
    "    pi = np.zeros(num_labels, dtype=int)\n",
    "    theta = np.zeros((num_labels, num_features))\n",
    "    pi_log_denom = math.log(num_documents + num_labels * l)\n",
    "    i = 0\n",
    "    for (label, (n, sum_term_freq)) in aggreagated:\n",
    "        labels[i] = label\n",
    "        pi[i] = math.log(n + l) - pi_log_denom\n",
    "        sum_term_freq_dense = sum_term_freq.toarray()\n",
    "        theta_log_denom = math.log(sum_term_freq.sum() + num_features * l)\n",
    "        theta[i,:] = np.log(sum_term_freq_dense + l) - theta_log_denom\n",
    "        i += 1  \n",
    "    return MLNaiveBayesModel(labels, pi, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_idx = dict([(l, i) for i,l in enumerate(top_1000_labels.keys())])\n",
    "label_idx_br = sc.broadcast(labels_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    doc = x[0]\n",
    "    labels = x[1]\n",
    "    features = sparse_vector(doc.toArray())\n",
    "    labels = [label_idx_br.value[l] for l in labels]\n",
    "    return {'lables':labels, 'features':features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e3d518406931>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAccuracyMetricWithStat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_n\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintersect_n\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pred_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_n\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_intersect_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mintersect_n\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Metric' is not defined"
     ]
    }
   ],
   "source": [
    "class AccuracyMetricWithStat(Metric):\n",
    "    def __init__(self, pred_n: int, intersect_n: int):\n",
    "        self._pred_n = pred_n\n",
    "        self._intersect_n = intersect_n\n",
    "        self.stats = []\n",
    "        super(AccuracyMetric, self).__init__(name='Accuracy', verbose=False)\n",
    "    \n",
    "    def evaluate(self, lables_and_predictions: RDD):\n",
    "        TP = lables_and_predictions.map(lambda x:\n",
    "                                    (set(x[0]), set([p for p,w in x[1][:self._pred_n]]))). \\\n",
    "                                    filter(lambda x:\n",
    "                                           len(x[0].intersection(x[1])) >= self._intersect_n)\n",
    "        accuracy = 100.0 * TP.count() / lables_and_predictions.count()\n",
    "        \n",
    "        stat = lables_and_predictions.map(lambda x:\n",
    "                                    (set(x[0]), set([p for p,w in x[1][:self._pred_n]]))). \\\n",
    "                                    map(lambda x:\n",
    "                                           (len(x[0].intersection(x[1])), len(x[0]))). \\\n",
    "            map(lambda x: _calc_sim(x[0], x[1], self._pred_n)).collect()\n",
    "        self.stats.append(stat)\n",
    "            \n",
    "        if self._verbose:\n",
    "            print('accuracy: ', accuracy)\n",
    "        self._results.append(accuracy)\n",
    "        return accuracy\n",
    "    \n",
    "    def _calc_sim(interselction, label_count, pred_n):\n",
    "        return intersection / (label_count if label_count <= pred_n else pred_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect_data = new_data.map(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric_3 = AccuracyMetric(3, 1)\n",
    "metric_5 = AccuracyMetric(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = hold_out(sc, vect_data, 4, train_model, [metric_3, metric_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[87.56090285373372, 87.14550238768152, 86.57375601138483, 87.7895752895753]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1].results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80.4757872045577, 79.31957186544342, 80.21952539228569, 80.44751464183811]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[53] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_3 = hold_out(sc, vect_data, 4, train_model, [AccuracyMetric(3, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78.56844159464734, 79.10375732506033, 78.5027329966097, 78.61372344130965]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_3[0].results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.69716383940676"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([78.56844159464734, 79.10375732506033, 78.5027329966097, 78.61372344130965]) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.73773214415178"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([79.21065341889188, 78.67745339430179, 77.92459356624005, 79.13822819717339]) / 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
