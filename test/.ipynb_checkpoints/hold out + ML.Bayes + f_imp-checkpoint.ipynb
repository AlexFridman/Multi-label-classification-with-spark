{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import json\n",
    "from pyspark import RDD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.addPyFile('/home/hadoop/spark/lib/sparse.py')\n",
    "sc.addPyFile('/home/hadoop/spark/lib/model.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparse import sparse_vector\n",
    "np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model import MLNaiveBayesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_and_split(data: RDD, fold_n: int, seed = 0):\n",
    "    fold_weights = [1 / fold_n] * fold_n\n",
    "    return data.randomSplit(fold_weights)\n",
    "\n",
    "def hold_out(sc, data: RDD, k: int, model_builder, metrics: list):\n",
    "    folds = shuffle_and_split(data, k)\n",
    "    for i in range(k):\n",
    "        test = folds[i]\n",
    "        training = sc.union(folds[:i] + folds[i + 1:])\n",
    "        model = model_builder(training)\n",
    "        model_broadcast = sc.broadcast(model)\n",
    "        lables_and_predictions = test.map(lambda x: (x['lables'],\n",
    "                                      model_broadcast.value.predict_all(x['features'])))\n",
    "        for metric in metrics:\n",
    "            metric.evaluate(lables_and_predictions)\n",
    "    return metrics\n",
    "\n",
    "class Metric:\n",
    "    def __init__(self, name: str, verbose=False):\n",
    "        self._name = name\n",
    "        self._results = []\n",
    "        self._verbose = verbose\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def results(self):\n",
    "        return self._results\n",
    "    \n",
    "    @property\n",
    "    def avg(self):\n",
    "        return np.average(self._results)\n",
    "    \n",
    "    def evaluate(self, lables, predictions):\n",
    "        pass\n",
    "\n",
    "class AccuracyMetric(Metric):\n",
    "    def __init__(self, pred_n: int, intersect_n: int):\n",
    "        self._pred_n = pred_n\n",
    "        self._intersect_n = intersect_n\n",
    "        super(AccuracyMetric, self).__init__(name='Accuracy', verbose=False)\n",
    "        \n",
    "    def evaluate(self, lables_and_predictions: RDD):\n",
    "        TP = lables_and_predictions.map(lambda x:\n",
    "                                    (set(x[0]), set([p for p,w in x[1][:self._pred_n]]))). \\\n",
    "                                    filter(lambda x:\n",
    "                                           len(x[0].intersection(x[1])) >= self._intersect_n)\n",
    "        accuracy = 100.0 * TP.count() / lables_and_predictions.count()\n",
    "        if self._verbose:\n",
    "            print('accuracy: ', accuracy)\n",
    "        self._results.append(accuracy)\n",
    "        return accuracy\n",
    "\n",
    "from pyspark.mllib.classification import NaiveBayesModel\n",
    "from pyspark.mllib.linalg import _convert_to_vector\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import RDD\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# RDD (labels) (features)\n",
    "def train_model(data, l = 1.0):\n",
    "    aggreagated = data.flatMap(lambda x: [(l, x['features']) for l in x['lables']]). \\\n",
    "        combineByKey(lambda v: (1, v),\n",
    "                 lambda c, v: (c[0] + 1, c[1] + v),\n",
    "                 lambda c1, c2: (c1[0] + c2[0], c1[1] + c2[1])). \\\n",
    "        sortBy(lambda x: x[0]). \\\n",
    "        collect()\n",
    "    num_labels = len(aggreagated)\n",
    "    num_documents = data.count()\n",
    "    num_features = aggreagated[0][1][1].size\n",
    "    labels = np.zeros(num_labels)\n",
    "    pi = np.zeros(num_labels, dtype=int)\n",
    "    theta = np.zeros((num_labels, num_features))\n",
    "    pi_log_denom = math.log(num_documents + num_labels * l)\n",
    "    i = 0\n",
    "    for (label, (n, sum_term_freq)) in aggreagated:\n",
    "        labels[i] = label\n",
    "        pi[i] = math.log(n + l) - pi_log_denom\n",
    "        sum_term_freq_dense = sum_term_freq.toarray()\n",
    "        theta_log_denom = math.log(sum_term_freq.sum() + num_features * l)\n",
    "        theta[i,:] = np.log(sum_term_freq_dense + l) - theta_log_denom\n",
    "        i += 1  \n",
    "    return MLNaiveBayesModel(labels, pi, theta)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "MultilabledPoint = namedtuple('MultilabledPoint',\n",
    "                              ['lables', 'features'], verbose=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class MultilabledPoint(object):\n",
    "    def __init__(self, lables, features):\n",
    "        self.lables = lables\n",
    "        self.features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = MLUtils.loadLibSVMFile(sc, 'hdfs://master:54310/exp_f/l_svm_b_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse_vector_to_coo(vect):\n",
    "    return tuple([(int(i), vect[int(i)]) for i in vect.indices])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_data = raw_data.map(lambda lp: (sparse_vector_to_coo(lp.features), lp.label)).groupByKey() \\\n",
    "    .map(lambda g: {'features': g[0], 'labels': list(g[1])}).filter(lambda x: len(x['features']) > 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85782"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/hadoop/f_imp.json', 'r') as fp:\n",
    "    f_rating = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226119"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_features(doc, w_l):\n",
    "    old = doc['features']\n",
    "    new = [(w_l[i], int(v)) for i,v in old if i in w_l]\n",
    "    f_num = len(w_l)\n",
    "    doc['features'] = sparse_vector(new, length=f_num)\n",
    "    doc['lables'] = doc['labels']\n",
    "    del doc['labels']\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  90000 res: [ [79.38970416957838, 77.62520945820145, 77.82626862899856, 78.41175918098995] ] avg:  78.3132353594\n"
     ]
    }
   ],
   "source": [
    "f_count = 90000\n",
    "top_f = dict(zip(f_rating[:f_count], range(f_count)))\n",
    "top_f_br = sc.broadcast(top_f)\n",
    "new_data = temp_data.map(lambda x: filter_features(x, top_f_br.value)).filter(lambda x: len(x['features']) > 14)\n",
    "new_data.cache()\n",
    "metric = AccuracyMetric(5, 1)\n",
    "hold_out(sc, new_data, 4, train_model, [metric])\n",
    "new_data.unpersist()\n",
    "print('count: ', f_count, 'res: [', metric.results, '] avg: ', metric.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_count = 110000\n",
    "top_f = dict(zip(f_rating[:f_count], range(f_count)))\n",
    "top_f_br = sc.broadcast(top_f)\n",
    "new_data = temp_data.map(lambda x: filter_features(x, top_f_br.value)).filter(lambda x: len(x['features']) > 14)\n",
    "new_data.cache()\n",
    "metric = AccuracyMetric(5, 1)\n",
    "hold_out(sc, new_data, 4, train_model, [metric])\n",
    "new_data.unpersist()\n",
    "print('count: ', f_count, 'res: [', metric.results, '] avg: ', metric.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  130000 res: [ [77.63885661166628, 77.44606358819077, 76.95402830450467, 77.73025245246176] ] avg:  77.4423002392\n",
      "count:  140000 res: [ [77.36163900991934, 77.28337236533957, 77.58436118222015, 77.24699387077153] ] avg:  77.3690916071\n",
      "count: "
     ]
    }
   ],
   "source": [
    "for f_count in range(130000, 210000, 10000):\n",
    "    top_f = dict(zip(f_rating[:f_count], range(f_count)))\n",
    "    top_f_br = sc.broadcast(top_f)\n",
    "    new_data = temp_data.map(lambda x: filter_features(x, top_f_br.value)).filter(lambda x: len(x['features'])   > 14)\n",
    "    new_data.cache()\n",
    "    metric = AccuracyMetric(5, 1)\n",
    "    hold_out(sc, new_data, 4, train_model, [metric])\n",
    "    new_data.unpersist()\n",
    "    print('count: ', f_count, 'res: [', metric.results, '] avg: ', metric.avg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "count:  10000 res: [ [77.0018402302647, 75.3196633654159, 75.68246291215179, 75.08225589693684] ] avg:  75.7715556012\n",
    "count:  20000 res: [ [78.60486674391657, 77.38900732227269, 76.67641233613128, 76.82864211361621] ] avg:  77.374732129\n",
    "count:  30000 res: [ [78.9037395799376, 79.30168766562834, 79.34812888847618, 79.56918446053993] ] avg:  79.2806851486\n",
    "count:  40000 res: [ [79.48861777216847, 79.32156133828997, 79.53319694997211, 78.93357052572445] ] avg:  79.3192366465\n",
    "count:  50000 res: [ [79.8510259533402, 79.35822420358502, 79.54194905351717, 79.17887199516437] ] avg:  79.4825178014\n",
    "count:  60000 res: [ [84.4306340825195, 82.42827868852459, 78.98547361581659, 77.40380114221514] ] avg:  80.8120468823\n",
    "count:  70000 res: [ [85.38386056726321, 85.1691159318871, 85.73030034097809, 85.48544365510517] ] avg:  85.4421801238\n",
    "count:  80000 res: [ [78.36949375410914, 78.64908628267406, 78.97107303709895, 78.63704536932883] ] avg:  78.6566746108\n",
    "count:  90000 res: [ [78.02546285501096, 78.41153882176641, 78.65472525422147, 77.93196268622081] ] avg:  78.2559224043\n",
    "count:  100000 res: [ [78.77448471595403, 77.30110302745834, 78.640059127864, 78.10273715785527] ] avg:  78.2045960073\n",
    "count:  110000 res: [ [78.47680546982575, 77.6918445792618, 77.97510837645085, 77.57785467128028] ] avg:  77.9304032742\n",
    "count:  120000 res: [ [77.82356246488106, 77.99177867073115, 77.52677061807252, 78.13620071684588] ] avg:  77.8695781176\n",
    "count:  130000 res: [ [77.63885661166628, 77.44606358819077, 76.95402830450467, 77.73025245246176] ] avg:  77.4423002392\n",
    "count:  140000 res: [ [77.36163900991934, 77.28337236533957, 77.58436118222015, 77.24699387077153] ] avg:  77.3690916071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = AccuracyMetric(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = hold_out(sc, vect_data, 4, train_model, [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65.01506024096386, 64.63493645651633, 64.69965732715178, 65.3236982775688]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric_2 = AccuracyMetric(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_2 = hold_out(sc, vect_data, 3, train_model, [metric_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[82.13314130260842, 81.90346375881975, 81.87474747474748]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_2.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CV мин длины текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(0, 100, 10):\n",
    "    top_1000 = raw_data.flatMap(lambda x: x['Labels']). \\\n",
    "    map(lambda l: (l,1)). \\\n",
    "    reduceByKey(int.__add__). \\\n",
    "    sortBy(lambda lc: lc[1], ascending=False). \\\n",
    "    map(lambda lc: lc[0]). \\\n",
    "    take(1000)\n",
    "    \n",
    "    top_1000_set = set(top_1000)\n",
    "    top_1000_set_br = sc.broadcast(top_1000_set)\n",
    "\n",
    "    data = raw_data.map(filter_lables). \\\n",
    "    filter(lambda x: len(x['Labels']) > 2)\n",
    "    \n",
    "    word_idx = data.flatMap(lambda x: set(x['Features'])). \\\n",
    "    distinct(). \\\n",
    "    zipWithIndex(). \\\n",
    "    collectAsMap()\n",
    "    label_idx = data.flatMap(lambda x: x['Labels']). \\\n",
    "    distinct(). \\\n",
    "    zipWithIndex(). \\\n",
    "    collectAsMap()\n",
    "    num_features = len(word_idx)\n",
    "    \n",
    "    word_idx_br = sc.broadcast(word_idx)\n",
    "    label_idx_br = sc.broadcast(label_idx)\n",
    "    \n",
    "    vect_data = data.filter(lambda x: len(x['Features']) > i). \\\n",
    "    map(vectorize_data)\n",
    "    temp = hold_out(sc, vect_data, 4, train_model, [AccuracyMetric(3, 1)])\n",
    "    print(temp[0].avg)\n",
    "    result.extend(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, testing = vect_data.randomSplit([0.7, 0.3], seed=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLNaiveBayesModel(model.labels, model.pi, model.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.8570557385047"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(p, model):\n",
    "    lables = p['lables']\n",
    "    features = p['features']\n",
    "    prediction = model.predict_all(features)[:3]\n",
    "    pred_lables = [l for l,w in prediction]\n",
    "    return (set(lables), set(pred_lables))\n",
    "    \n",
    "testing.repartition(12). \\\n",
    "    map(lambda p: predict(p, model)). \\\n",
    "    filter(lambda x: len(x[1].intersection(x[0])) > 0). \\\n",
    "    count() / testing.count() * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
