{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import json\n",
    "from pyspark import RDD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.addPyFile('/home/hadoop/spark/lib/sparse.py')\n",
    "sc.addPyFile('/home/hadoop/spark/lib/model.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparse import sparse_vector\n",
    "np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model import MLNaiveBayesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_and_split(data: RDD, fold_n: int, seed = 0):\n",
    "    fold_weights = [1 / fold_n] * fold_n\n",
    "    return data.randomSplit(fold_weights)\n",
    "\n",
    "def hold_out(sc, data: RDD, k: int, model_builder, metrics: list):\n",
    "    folds = shuffle_and_split(data, k)\n",
    "    for i in range(k):\n",
    "        test = folds[i]\n",
    "        training = sc.union(folds[:i] + folds[i + 1:])\n",
    "        model = model_builder(training)\n",
    "        model_broadcast = sc.broadcast(model)\n",
    "        lables_and_predictions = test.map(lambda x: (x['lables'],\n",
    "                                      model_broadcast.value.predict_all(x['features'])))\n",
    "        for metric in metrics:\n",
    "            metric.evaluate(lables_and_predictions)\n",
    "    return metrics\n",
    "\n",
    "class Metric:\n",
    "    def __init__(self, name: str, verbose=False):\n",
    "        self._name = name\n",
    "        self._results = []\n",
    "        self._verbose = verbose\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def results(self):\n",
    "        return self._results\n",
    "    \n",
    "    @property\n",
    "    def avg(self):\n",
    "        return np.average(self._results)\n",
    "    \n",
    "    def evaluate(self, lables, predictions):\n",
    "        pass\n",
    "\n",
    "class AccuracyMetric(Metric):\n",
    "    def __init__(self, pred_n: int, intersect_n: int):\n",
    "        self._pred_n = pred_n\n",
    "        self._intersect_n = intersect_n\n",
    "        super(AccuracyMetric, self).__init__(name='Accuracy', verbose=False)\n",
    "        \n",
    "    def evaluate(self, lables_and_predictions: RDD):\n",
    "        TP = lables_and_predictions.map(lambda x:\n",
    "                                    (set(x[0]), set([p for p,w in x[1][:self._pred_n]]))). \\\n",
    "                                    filter(lambda x:\n",
    "                                           len(x[0].intersection(x[1])) >= self._intersect_n)\n",
    "        accuracy = 100.0 * TP.count() / lables_and_predictions.count()\n",
    "        if self._verbose:\n",
    "            print('accuracy: ', accuracy)\n",
    "        self._results.append(accuracy)\n",
    "        return accuracy\n",
    "\n",
    "from pyspark.mllib.classification import NaiveBayesModel\n",
    "from pyspark.mllib.linalg import _convert_to_vector\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import RDD\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# RDD (labels) (features)\n",
    "def train_model(data, l = 1.0):\n",
    "    aggreagated = data.flatMap(lambda x: [(l, x['features']) for l in x['lables']]). \\\n",
    "        combineByKey(lambda v: (1, v),\n",
    "                 lambda c, v: (c[0] + 1, c[1] + v),\n",
    "                 lambda c1, c2: (c1[0] + c2[0], c1[1] + c2[1])). \\\n",
    "        sortBy(lambda x: x[0]). \\\n",
    "        collect()\n",
    "    num_labels = len(aggreagated)\n",
    "    num_documents = data.count()\n",
    "    num_features = aggreagated[0][1][1].size\n",
    "    labels = np.zeros(num_labels)\n",
    "    pi = np.zeros(num_labels, dtype=int)\n",
    "    theta = np.zeros((num_labels, num_features))\n",
    "    pi_log_denom = math.log(num_documents + num_labels * l)\n",
    "    i = 0\n",
    "    for (label, (n, sum_term_freq)) in aggreagated:\n",
    "        labels[i] = label\n",
    "        pi[i] = math.log(n + l) - pi_log_denom\n",
    "        sum_term_freq_dense = sum_term_freq.toarray()\n",
    "        theta_log_denom = math.log(sum_term_freq.sum() + num_features * l)\n",
    "        theta[i,:] = np.log(sum_term_freq_dense + l) - theta_log_denom\n",
    "        i += 1  \n",
    "    return MLNaiveBayesModel(labels, pi, theta)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "MultilabledPoint = namedtuple('MultilabledPoint',\n",
    "                              ['lables', 'features'], verbose=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class MultilabledPoint(object):\n",
    "    def __init__(self, lables, features):\n",
    "        self.lables = lables\n",
    "        self.features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile('hdfs://master:54310/excluded_fin').map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "top_2000 = raw_data.flatMap(lambda x: x['Labels']). \\\n",
    "    map(lambda l: (l,1)). \\\n",
    "    reduceByKey(int.__add__). \\\n",
    "    sortBy(lambda lc: lc[1], ascending=False). \\\n",
    "    map(lambda lc: lc[0]). \\\n",
    "    take(2000)\n",
    "eng_from_top_2000 = [l for l in top_2000 if re.match('.*([a-z]).*', l)]\n",
    "with open('/home/hadoop/data/eng_from_top_2000.txt', 'w+') as fp:\n",
    "    for l in sorted(eng_from_top_2000):\n",
    "        fp.write(l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/hadoop/data/top_2000.txt', 'w+') as fp:\n",
    "    for l in sorted(top_2000):\n",
    "        fp.write(l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_1000 = raw_data.flatMap(lambda x: x['Labels']). \\\n",
    "    map(lambda l: (l,1)). \\\n",
    "    reduceByKey(int.__add__). \\\n",
    "    sortBy(lambda lc: lc[1], ascending=False). \\\n",
    "    map(lambda lc: lc[0]). \\\n",
    "    take(1000)\n",
    "#top_1000[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_1000_set = set(top_1000)\n",
    "top_1000_set_br = sc.broadcast(top_1000_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_lables(obj: dict):\n",
    "    obj['Labels'] = [l for l in obj['Labels'] if l in top_1000_set_br.value]\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = raw_data.map(filter_lables). \\\n",
    "    filter(lambda x: len(x['Labels']) > 2)\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_idx = data.flatMap(lambda x: set(x['Features'])). \\\n",
    "    distinct(). \\\n",
    "    zipWithIndex(). \\\n",
    "    collectAsMap()\n",
    "label_idx = data.flatMap(lambda x: x['Labels']). \\\n",
    "    distinct(). \\\n",
    "    zipWithIndex(). \\\n",
    "    collectAsMap()\n",
    "num_features = len(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idx_br = sc.broadcast(word_idx)\n",
    "label_idx_br = sc.broadcast(label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_words(words: list):\n",
    "    word_counts = Counter([word_idx_br.value[w] for w in words])\n",
    "    return sparse_vector(list(word_counts.items()), length=num_features, dtype=np.int32)\n",
    "\n",
    "def vectorize_data(x: dict):\n",
    "    features = vectorize_words(x['Features'])\n",
    "    lables = [label_idx_br.value[l] for l in x['Labels']]\n",
    "    return {'lables':lables, 'features':features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58843"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_data = data.map(vectorize_data)\n",
    "#vect_data.cache()\n",
    "vect_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[123] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels = sc.textFile('hdfs://master:54310/raw_data'). \\\n",
    "    map(json.loads). \\\n",
    "    flatMap(lambda x: x['Hubs'] + x['Tags']).distinct()\n",
    "unique_labels.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find(pattern):\n",
    "    regexp = re.compile('.*('+pattern+').*')\n",
    "    return unique_labels.filter(lambda l: regexp.match(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scm-manager',\n",
       " 'managedblocker',\n",
       " 'manager',\n",
       " 'googe tag manager',\n",
       " 'managed code',\n",
       " 'mobile device manager',\n",
       " 'coremanager',\n",
       " 'system management mode',\n",
       " 'ifxmanager',\n",
       " 'process manager',\n",
       " 'compliance management',\n",
       " 'window manager',\n",
       " 'risk managment',\n",
       " 'package management',\n",
       " 'clents management',\n",
       " 'application performance management',\n",
       " 'billmanager',\n",
       " 'intel node manager',\n",
       " 'test management',\n",
       " 'sales management']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find('manag').take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[48] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_data = data.filter(lambda x: len(x['Features']) > 19). \\\n",
    "    map(vectorize_data)\n",
    "vect_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = vect_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.57375167400038"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(p, model):\n",
    "    lables = p['lables']\n",
    "    features = p['features']\n",
    "    prediction = model.predict_all(features)[:3]\n",
    "    pred_lables = [l for l,w in prediction]\n",
    "    return (set(lables), set(pred_lables))\n",
    "    \n",
    "test.repartition(12). \\\n",
    "    map(lambda p: predict(p, model)). \\\n",
    "    filter(lambda x: len(x[1].intersection(x[0])) > 0). \\\n",
    "    count() / test.count() * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = hold_out(sc, vect_data, 4, train_model, [AccuracyMetric(3, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CV мин длины текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(0, 100, 10):\n",
    "    top_1000 = raw_data.flatMap(lambda x: x['Labels']). \\\n",
    "    map(lambda l: (l,1)). \\\n",
    "    reduceByKey(int.__add__). \\\n",
    "    sortBy(lambda lc: lc[1], ascending=False). \\\n",
    "    map(lambda lc: lc[0]). \\\n",
    "    take(1000)\n",
    "    \n",
    "    top_1000_set = set(top_1000)\n",
    "    top_1000_set_br = sc.broadcast(top_1000_set)\n",
    "\n",
    "    data = raw_data.map(filter_lables). \\\n",
    "    filter(lambda x: len(x['Labels']) > 2)\n",
    "    \n",
    "    word_idx = data.flatMap(lambda x: set(x['Features'])). \\\n",
    "    distinct(). \\\n",
    "    zipWithIndex(). \\\n",
    "    collectAsMap()\n",
    "    label_idx = data.flatMap(lambda x: x['Labels']). \\\n",
    "    distinct(). \\\n",
    "    zipWithIndex(). \\\n",
    "    collectAsMap()\n",
    "    num_features = len(word_idx)\n",
    "    \n",
    "    word_idx_br = sc.broadcast(word_idx)\n",
    "    label_idx_br = sc.broadcast(label_idx)\n",
    "    \n",
    "    vect_data = data.filter(lambda x: len(x['Features']) > i). \\\n",
    "    map(vectorize_data)\n",
    "    temp = hold_out(sc, vect_data, 4, train_model, [AccuracyMetric(3, 1)])\n",
    "    print(temp[0].avg)\n",
    "    result.extend(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, testing = vect_data.randomSplit([0.7, 0.3], seed=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLNaiveBayesModel(model.labels, model.pi, model.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.8570557385047"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(p, model):\n",
    "    lables = p['lables']\n",
    "    features = p['features']\n",
    "    prediction = model.predict_all(features)[:3]\n",
    "    pred_lables = [l for l,w in prediction]\n",
    "    return (set(lables), set(pred_lables))\n",
    "    \n",
    "testing.repartition(12). \\\n",
    "    map(lambda p: predict(p, model)). \\\n",
    "    filter(lambda x: len(x[1].intersection(x[0])) > 0). \\\n",
    "    count() / testing.count() * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
